# DecisionTreeID3

Implementa√ß√£o pura do algoritmo ID3 para atributos **categ√≥ricos**.

## üìù Algoritmo ID3 em detalhes

1. **Node** (`Node`):  
   - `feature` (√≠ndice)  
   - `category` (valor testado)  
   - `left` / `right` (sub√°rvores)  
   - `value` (classe majorit√°ria em n√≥s-folha)

2. **Fit** (`fit(X, y)`):  
   - Converte `X`/`y` para `ndarray`  
   - Garante `X` em 2D  
   - Define `n_features` (padr√£o = todas)  
   - Chama `_grow_tree`

3. **Crescimento recursivo** (`_grow_tree`):  
   - Crit√©rios de parada:  
     - `n_samples < min_samples_split`  
     - `depth >= max_depth`  
     - todas as inst√¢ncias t√™m mesma classe  
     - **melhor ganho ‚â§ 0**  
   - Em cada n√≥:  
     - Amostra aleat√≥ria de `n_features` √≠ndices  
     - Para cada `(feature, categoria)`:  
       - Particiona em `==` vs `!=`  
       - Calcula **Information Gain** (diferen√ßa de entropia)  
     - Escolhe `best_feat` + `best_cat`  
     - Se `best_gain > 0`, cria `Node` e recursa para `left`/`right`; sen√£o, n√≥-folha.

4. **Entropia** (`_entropy(y)`):  
   - Usa `Counter` para contar frequ√™ncias  
   - \(-\sum p_i \log p_i\)

5. **Information Gain** (`_information_gain(col, y, cat)`):  
   - Entropia do pai ‚àí entropia ponderada dos filhos

6. **Split categ√≥rico** (`_split(col, cat)`):  
   - Igual vs diferente

7. **Predict** (`predict(X)`):  
   - Travessa cada amostra do n√≥ raiz at√© n√≥-folha seguindo `==`/`!=`

## üíª Exemplo de Uso

```py
from id3_decision_tree import DecisionTreeID3

clf = DecisionTreeID3(max_depth=5, min_samples_split=10)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)